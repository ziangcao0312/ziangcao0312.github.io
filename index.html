
<!doctype html>
<html lang="en">
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ziangcao - Homepage</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
      .social-row {
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
   <h3><img src="IMG_2409_edit_pro_photo_square_small.jpg" height="150" width="150" alt="Ziang Cao"></h3>
        <h1>Ziang Cao</h1>
        <p>Undergraduate student<br>Tongji University</p>
    <h3><a href="https://ziangcao0312.github.io/research/CV.pdf">CV</a></h3>  

    <b>Social</b><br>
        <div class="social-row">
          <a href="mailto:1753419@tongji.edu.cn" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
          <a href="https://scholar.google.com/citations?user=L9tbNTsAAAAJ&hl=zh-CN" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
          <a href="https://orcid.org/0000-0002-5682-9446"><i class="ai ai-fw ai-orcid-square"></i> ORCID</a><br>
          <a href="https://github.com/ziangcao0312"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
          <a href="https://twitter.com/ziangcao_" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
          <br>
        </div>
        <br>
      <b>Interests </b><br>
      <p>Deep learning, Robotics, Computer vision.</p>

      </header>
      <section>
    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>About me</h2>
    <p style="margin:0">Ziang Cao is currently a undergraduate student in the School of Vehicle Engineering at Tongji Unviersity. His research interests lie on the computer vision and deep learning. <br> 
   <h1> <h1>
 <hr>

    <h2><a id="works-in-progress" class="anchor" href="#workinprogress" aria-hidden="true"><span class="octicon octicon-link"></span></a>News</h2>
<!--
    <p style="margin:0"> <b>How Substitutable are Native- and Foreign-born Workers? Wage Effects from STEM In-flow</b> <br> with <a href="https://sites.google.com/site/johnvwinters/">John V. Winters</a>. <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> We study heterogeneity in the effect of immigrant labor supply on native wages in the United States using quasi-experimental variation induced by the Immigration Act of 1990. The Act quickly and substantially increased the number of immigrant workers in STEM occupations. Using CPS data, we find short-run policy effects of reduced wages for more substitutable workers but increased wages for more complementary workers. We analyze long-term effects of the policy using administrative earnings data and find smaller long-run effects. </p></div><br>
-->

    <p style="margin:0"> <b>[Aug. 2021] Two papers are accepted by <a href="https://cvpr2022.thecvf.com/">CVPR2022.</a></b> 
    <p style="margin:0"> <b>[Aug. 2021] One paper is accepted by <a href="https://ieeexplore.ieee.org/abstract/document/9696362/">RA-L2022.</a></b> 
    <p style="margin:0"> <b>[Aug. 2021] One paper is accepted by <a href="https://arxiv.org/abs/2203.01516">ICRA2022.</a></b> 
    <p style="margin:0"> <b>[Aug. 2021] One paper is accepted by <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Cao_HiFT_Hierarchical_Feature_Transformer_for_Aerial_Tracking_ICCV_2021_paper.html">ICCV2021.</a></b> 
    <p style="margin:0"> <b>[July. 2021] Two papers (<a href="https://arxiv.org/abs/2106.08816">1</a> and <a href="https://arxiv.org/abs/2107.14389">2</a> ) are accepted by IROS2021.</b> 
    <p style="margin:0"> <b>[May. 2021] One paper is accepted by <a href="https://ieeexplore.ieee.org/abstract/document/9477413">TGRS.</a></b> 
    <p style="margin:0"> <b>[Feb. 2021] One paper is accepted by <a href="https://ieeexplore.ieee.org/abstract/document/9560756/">ICRA2021.</a></b> 

   <h1> <h1>
 <hr>



    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Publications</h2>
    
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ai4ce.github.io/EgoPAT3D/">Egocentric Prediction of Action Target in 3D</a> <br> Yiming Li*,<b>Ziang Cao*</b>, Andrew Liang, Benjamin Liang, Luoyao Chen, Hang Zhao, Chen Feng. <i>CVPR2022</i>, <b>* denotes equal contribution</b>.  <br><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>We are interested in anticipating as early as possible the target location of a person's object manipulation action in a 3D workspace from egocentric vision. It is important in fields like human-robot collaboration, but has not yet received enough attention from vision and learning communities. To stimulate more research on this challenging egocentric vision task, we propose a large multi-modal dataset of more than 1 million frames of RGBD and IMU streams, and provide evaluation metrics based on our high-quality 2D and 3D labels from semi-automatic annotation. Meanwhile, we design baseline methods using recurrent neural networks (RNNs) and conduct various ablation studies to validate their effectiveness. Our results demonstrate that this new task is worthy of further study by researchers in robotics, vision, and learning communities. </p></div>
    <br>
    
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/2203.01885">TCTrack: Temporal Contexts for Aerial Tracking</a> <br> <b>Ziang Cao</b>, Ziyuan Huang, Liang Pan, Shiwei Zhang, Ziwei Liu, Changhong Fu. <i>CVPR2022</i>.  <br><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Temporal contexts among consecutive frames are far from being fully utilized in existing visual trackers. In this work, we present TCTrack, a comprehensive framework to fully exploit temporal contexts for aerial tracking. The temporal contexts are incorporated at \textbf{two levels}: the extraction of \textbf{features} and the refinement of \textbf{similarity maps}. Specifically, for feature extraction, an online temporally adaptive convolution is proposed to enhance the spatial features using temporal information, which is achieved by dynamically calibrating the convolution weights according to the previous frames. For similarity map refinement, we propose an adaptive temporal transformer, which first effectively encodes temporal knowledge in a memory-efficient way, before the temporal knowledge is decoded for accurate adjustment of the similarity map. TCTrack is effective and efficient: evaluation on four aerial tracking benchmarks shows its impressive performance; real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX Xavier. </p></div>
    <br>

    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://openaccess.thecvf.com/content/ICCV2021/html/Cao_HiFT_Hierarchical_Feature_Transformer_for_Aerial_Tracking_ICCV_2021_paper.html">HiFT: Hierarchical Feature Transformer for Aerial Tracking</a> <br> <b>Ziang Cao</b>, Changhong Fu, Junjie Ye, Bowen Li, Yiming Li. <i>ICCV2021</i>.  <br><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Most existing Siamese-based tracking methods execute the classification and regression of the target object based on the similarity maps. However, they either employ a single map from the last convolutional layer which degrades the localization accuracy in complex scenarios or separately use multiple maps for decision making, introducing intractable computations for aerial mobile platforms. Thus, in this work, we propose an efficient and effective hierarchical feature transformer (HiFT) for aerial tracking. Hierarchical similarity maps generated by multi-level convolutional layers are fed into the feature transformer to achieve the interactive fusion of spatial (shallow layers) and semantics cues (deep layers). Consequently, not only the global contextual information can be raised, facilitating the target search, but also our end-to-end architecture with the transformer can efficiently learn the interdependencies among multi-level features, thereby discovering a tracking-tailored feature space with strong discriminability. Comprehensive evaluations on four aerial benchmarks have proven the effectiveness of HiFT. Real-world tests on the aerial platform have strongly validated its practicability with a real-time speed. Our code is available at https://github.com/vision4robotics/HiFT. </p></div>
    <br>
    

    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/9477413">SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking</a> <br> <b>Ziang Cao</b>, Changhong Fu, Junjie Ye, Bowen Li, Yiming Li. <i>IROS2021</i>.  <br><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Recently, the Siamese-based method has stood out from multitudinous tracking methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to various special challenges in UAV tracking, \textit{e.g.}, severe occlusion, and fast motion, most existing Siamese-based trackers hardly combine superior performance with high efficiency. To this concern, in this paper, a novel attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking. By virtue of the attention mechanism, the attentional aggregation network (AAN) is conducted with self-AAN and cross-AAN, raising the expression ability of features eventually. The former AAN aggregates and models the self-semantic interdependencies of the single feature map via spatial and channel dimensions. The latter aims to aggregate the cross-interdependencies of different semantic features including the location information of anchors. In addition, the dual features version of the anchor proposal network is proposed to raise the robustness of proposing anchors, increasing the perception ability to objects with various scales. Experiments on two well-known authoritative benchmarks are conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA trackers. Besides, real-world tests onboard a typical embedded platform demonstrate that SiamAPN++ achieves promising tracking results with real-time speed. </p></div>
    <br>
    

    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/9477413">Onboard Real-Time Aerial Tracking With Efficient Siamese Anchor Proposal Network</a> <br> Changhong Fu, <b>Ziang Cao</b>, Yiming Li, Junjie Ye, Chen Feng. <i>TGRS</i>.  <br><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Object tracking approaches based on the Siamese network have demonstrated their huge potential in the remote sensing field recently. Nevertheless, due to the limited computing resource of aerial platforms and special challenges in aerial tracking, most existing Siamese-based methods can hardly meet the real-time and state-of-the-art performance simultaneously. Consequently, a novel Siamese-based method is proposed in this work for onboard real-time aerial tracking, i.e., SiamAPN. The proposed method is a no-prior two-stage method, i.e., Stage-1 for proposing adaptive anchors to enhance the ability of object perception and Stage-2 for fine-tuning the proposed anchors to obtain accurate results. Distinct from the traditional predefined anchors, the proposed anchors can adapt automatically to the tracking object. Besides, the internal information of adaptive anchors is utilized to feedback SiamAPN for enhancing the object perception. Attributing to the feature fusion network, different semantic information is integrated, enriching the information flow that is significant for robust aerial tracking. In the end, the regression and multiclassification operation refine the proposed anchors meticulously. Comprehensive evaluations on three well-known aerial tracking benchmarks have proven the superior performance of the presented approach. Moreover, to verify the practicability of the proposed method, SiamAPN is implemented onboard a typical embedded aerial tracking platform to conduct the real-world evaluations on specific aerial tracking scenarios, e.g., fast motion, long-term tracking, and low resolution. The results have demonstrated the efficiency and accuracy of the proposed approach, with a processing speed of over 30 frames/s. In addition, the image sequences in the real-world evaluations are collected and annotated as a new aerial tracking benchmark, i.e., UAVTrack112. </p></div>
    <br>


    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/abstract/document/9560756/">Siamese Anchor Proposal Network for High-Speed Aerial Tracking</a> <br> Changhong Fu, <b>Ziang Cao</b>, Yiming Li, Junjie Ye, Chen Feng. <i>ICRA2021</i>.  <br><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency. Therefore, their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV) is impeded. In this work, a novel two-stage Siamese network-based method is proposed for aerial tracking, i.e., stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) increase the robustness and generalization to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the superior performance of our approach, with a speed of âˆ¼200 frames/s. </p></div>
    <br>


    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://arxiv.org/abs/2107.14389">DarkLighter: Light Up the Darkness for UAV Tracking</a> <br> Junjie Ye, Changhong Fu, Guangze Zheng, <b>Ziang Cao</b>,, Bowen Li. <i>IROS2021</i>.  <br><button class="accordion"> 
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, i.e., ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability. </p></div>
    <br>
       


<h1> <h1>
 <hr>
<h2><a id="works-in-progress" class="anchor" href="#workinprogress" aria-hidden="true"><span class="octicon octicon-link"></span></a>Education & Experiences</h2>
<!--
    <p style="margin:0"> <b>How Substitutable are Native- and Foreign-born Workers? Wage Effects from STEM In-flow</b> <br> with <a href="https://sites.google.com/site/johnvwinters/">John V. Winters</a>. <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> We study heterogeneity in the effect of immigrant labor supply on native wages in the United States using quasi-experimental variation induced by the Immigration Act of 1990. The Act quickly and substantially increased the number of immigrant workers in STEM occupations. Using CPS data, we find short-run policy effects of reduced wages for more substitutable workers but increased wages for more complementary workers. We analyze long-term effects of the policy using administrative earnings data and find smaller long-run effects. </p></div><br>
-->

    <p style="margin:0"> <b>[Sep. 2017-Present] Tonji University</b> , Vehicle Engineering, Shanghai, China.

   <h1> <h1>
 <hr>


      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
